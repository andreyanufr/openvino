# auto generated by ir2py from /home/luocheng/models/gpt2_test/origin.xml

from openvino.runtime import Core, Model, Tensor, PartialShape, Type, Shape, serialize
from openvino.runtime.op import util as op_util
from openvino.runtime import opset10 as opset
from openvino.runtime.passes import Manager
import numpy as np
import sys, os
import argparse
import torch

MAX_SEQ_LENGTH = 128


def export_model_to_onnx(model, path):
    # MAX_SEQ_LENGTH = 128
    with torch.no_grad():
        default_input = torch.ones(1, MAX_SEQ_LENGTH, dtype=torch.int64)
        default_input = torch.zeros(1, MAX_SEQ_LENGTH, dtype=torch.int64)

        # inputs = {"input_ids": np.array(batch["input_ids"]),
        #         "attention_mask": np.array(batch["attention_mask"]),
        #         "token_type_ids": np.array(batch["token_type_ids"])}

        symbolic_names = {0: "batch_size", 1: "max_seq_len"}
        symbolic_names_past = {0: "batch_size", 1: "max_past_len"}

        values = model(default_input, use_cache=True)
        inputs = {
            "input_ids": default_input,
            "past_key_values": torch.stack([torch.stack(key_val) for key_val in values["past_key_values"]]),
        }
        torch.onnx.export(
            model,
            # {**inputs, "use_cache": True},
            # (inputs["input_ids"], inputs["past_key_values"], None, None, None, None, None, None, None, True),
            (inputs["input_ids"], None, None, None, inputs["past_key_values"], None, True),
            path,
            opset_version=12,
            do_constant_folding=True,
            input_names=["input_ids", "past_key_values"],
            output_names=["logits", "past_key_values_out"],
            dynamic_axes={
                "input_ids": symbolic_names,
                # "past_key_values": symbolic_names_past,
            },
        )

    print("ONNX model saved to {}".format(path))


def convert_to_ov(onnx_model_path, openvino_dir, model_name, has_token_type_ids=False):
    try:
        if has_token_type_ids:
            os.system(
                f'mo --input_model {onnx_model_path} --output_dir {openvino_dir} --model_name {model_name} --input input_ids,attention_mask,token_type_ids --input_shape [1,{MAX_SEQ_LENGTH}],[1,{MAX_SEQ_LENGTH}],[1,{MAX_SEQ_LENGTH}] --output logits --data_type FP16')
        else:
            # os.system(f'mo --input_model {onnx_model_path} --output_dir {openvino_dir} --model_name {model_name} --input input_ids,attention_mask --input_shape [1,{MAX_SEQ_LENGTH}],[1,{MAX_SEQ_LENGTH}] --output logits --data_type FP16')
            os.system(
                f'mo --input_model {onnx_model_path} --output_dir {openvino_dir} --model_name {model_name} --input input_ids,past_key_values --output logits,past_key_values --data_type FP16')  # --input_shape [1,{MAX_SEQ_LENGTH}]
    except:
        print(f"Error in ir convertion for model {onnx_model_path}")
        return


def convert_model_to_onnx_and_ov(model_id, onnx_path, ov_path):
    from transformers import AutoModelForCausalLM, AutoTokenizer
    import torch
    tokenizer = AutoTokenizer.from_pretrained(model_id)
    model = AutoModelForCausalLM.from_pretrained(model_id).to('cpu').eval()

    if not os.path.exists(onnx_path):
        os.makedirs(onnx_path)

    onnx_name = os.path.join(onnx_path, model_id.replace('/', '_') + ".onnx")
    export_model_to_onnx(model, onnx_name, False, has_attention=False)

    if not os.path.exists(ov_path):
        os.makedirs(ov_path)
    convert_to_ov(onnx_name, ov_path, model_id.replace('/', '_'))


if __name__ == "__main__":
    parser = argparse.ArgumentParser("")
    parser.add_argument("model")
    # parser.add_argument("ov_model_path")
    args = parser.parse_args()

    convert_model_to_onnx_and_ov(args.model, "gpt_neox_onnx", "gpt_neox_ov_model")